# quick_test.py (ç»ˆæç®€åŒ–ç‰ˆ - ç»å¯¹å¯é )
"""
è¿”ç’å½’çœŸæµ‹è¯•è„šæœ¬
- ç§»é™¤äº†æ‰€æœ‰å¤æ‚çš„ã€å¯èƒ½å¯¼è‡´æ­»é”çš„å®æ—¶æ—¥å¿—è¯»å–é€»è¾‘ã€‚
- ä½¿ç”¨æœ€ç¨³å®šå¯é çš„ subprocess è°ƒç”¨æ–¹å¼ã€‚
- ä¿ç•™äº†æ‰€æœ‰æ­£ç¡®çš„ç¯å¢ƒå˜é‡å’Œæ¸…ç†é€»è¾‘ã€‚
"""

import os
import sys
import subprocess
import time

# ===================== é…ç½® (ä¿æŒä¸å˜) =====================
NUM_GPUS = 2
CUDA_VISIBLE_DEVICES = "0,1"
MASTER_ADDR = "127.0.0.1"
MASTER_PORT = 29600
CSV_PATH = "/home/stat-jijianxin/PFMs/Survival_code/csv_file/hmu_survival_with_slides.csv"
H5_DIR = "/home/stat-jijianxin/PFMs/HMU_GC_ALL_H5"
EXTERNAL_CSV = "/home/stat-jijianxin/PFMs/Survival_code/csv_file/tcga_survival_matched.csv"
EXTERNAL_H5 = "/home/stat-jijianxin/PFMs/TRIDENT/tcga_filtered/20x_512px_0px_overlap"
TEST_PARAMS = {
    'dropout': 0.25, 'act': 'relu', 'mamba_layer': 2, 'batch_size': 4,
    'lr': 2e-4, 'weight_decay': 1e-5, 'optimizer': 'adamw',
    'ranking_weight': 0.1, 'gc': 1,
}

def cleanup_resources(deep=False):
    """æ¸…ç†èµ„æº (å·¥ä¸šçº§å¼ºåŒ–ç‰ˆ)"""
    print("ğŸ§¹ æ¸…ç†èµ„æº..." + (" (æ·±åº¦)" if deep else ""))
    subprocess.run("pkill -9 -f 'torchrun' 2>/dev/null || true", shell=True)
    subprocess.run("pkill -9 -f 'train_ddp.py' 2>/dev/null || true", shell=True)
    subprocess.run(f"fuser -k -n tcp {MASTER_PORT} 2>/dev/null || true", shell=True)
    
    print("  ğŸ§¹ æ¸…ç†å…±äº«å†…å­˜ (/dev/shm)...")
    subprocess.run("rm -rf /dev/shm/torch_* 2>/dev/null || true", shell=True)
    
    if deep:
        try:
            home_cache = os.path.expanduser("~/.cache")
            triton_cache = os.path.join(home_cache, "triton")
            if os.path.exists(triton_cache):
                print(f"  ğŸ§¹ æ¸…ç†Tritonç¼“å­˜ ({triton_cache})...")
                subprocess.run(f"rm -rf {triton_cache}/* 2>/dev/null || true", shell=True)
        except Exception:
            pass
    
    time.sleep(2)
    print("âœ“ æ¸…ç†å®Œæˆ")

def test_train_ddp():
    """æµ‹è¯• train_ddp.py - ä½¿ç”¨æœ€å¯é çš„å¯åŠ¨æ–¹å¼"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯• train_ddp.py (ç»ˆæç®€åŒ–ç‰ˆ)")
    print("="*60)
    
    cleanup_resources(deep=True)
    
    results_dir = "./test_results_1epoch"
    os.makedirs(results_dir, exist_ok=True)
    log_file = os.path.join(results_dir, "test.log")
    err_file = os.path.join(results_dir, "test.err")
    
    cmd = [
        "torchrun", f"--nproc_per_node={NUM_GPUS}", f"--master_addr={MASTER_ADDR}",
        f"--master_port={MASTER_PORT}", "train_ddp.py",
        "--csv_path", CSV_PATH, "--h5_dir", H5_DIR, "--external_csv_path", EXTERNAL_CSV,
        "--external_h5_dir", EXTERNAL_H5, "--feature_models", "ctranspath",
        "--batch_size", str(TEST_PARAMS['batch_size']), "--max_epochs", "1",
        "--lr", str(TEST_PARAMS['lr']), "--results_dir", results_dir,
        "--num_workers", "0", "--seed", "42",
        # ... å…¶ä»–å‚æ•°ä¿æŒä¸å˜ ...
        "--in_dim", "768", "--n_classes", "4", "--dropout", str(TEST_PARAMS['dropout']),
        "--act", TEST_PARAMS['act'], "--mamba_layer", str(TEST_PARAMS['mamba_layer']),
        "--weight_decay", str(TEST_PARAMS['weight_decay']), "--optimizer", TEST_PARAMS['optimizer'],
        "--gc", str(TEST_PARAMS['gc']), "--loss", "combined", "--main_loss_type", "nll",
        "--alpha_surv", "0.365", "--ranking_weight", str(TEST_PARAMS['ranking_weight']),
        "--ranking_margin", "0.0", "--k_fold", "3", "--fold", "0",
        "--val_ratio", "0.2", "--test_ratio", "0.2", "--warmup", "0",
        "--patience", "999", "--stop_epoch", "1",
    ]
    
    env = os.environ.copy()
    env['CUDA_VISIBLE_DEVICES'] = CUDA_VISIBLE_DEVICES
    env['NCCL_BLOCKING_WAIT'] = '1'
    env['NCCL_DEBUG'] = 'WARN'
    env['NCCL_TIMEOUT'] = '1800'
    env['NCCL_SHM_DISABLE'] = '0'
    env['NCCL_SOCKET_IFNAME'] = 'lo'
    env['NCCL_IB_DISABLE'] = '1'
    
    print(f"\nâš™ï¸  ç¯å¢ƒé…ç½®:")
    print(f"  - NCCL_BLOCKING_WAIT: {env['NCCL_BLOCKING_WAIT']} (âœ… é˜»å¡æ¨¡å¼)")
    print(f"  - æ—¥å¿—å°†ç›´æ¥å†™å…¥æ–‡ä»¶ï¼Œæ§åˆ¶å°å°†ä¿æŒå®‰é™ã€‚")
    print(f"\nğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
    print(f"ğŸ“ é”™è¯¯æ–‡ä»¶: {err_file}")
    print("\nğŸ”¥ å¼€å§‹æ‰§è¡Œ... (è¶…æ—¶ 5 åˆ†é’Ÿ)")
    print("\nğŸ’¡ è¯·æ‰“å¼€æ–°ç»ˆç«¯ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®æ—¶æŸ¥çœ‹æ—¥å¿—:")
    print(f"   tail -f {log_file}")
    
    process = None
    start_time = time.time()
    try:
        # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šä¸å†ä½¿ç”¨PIPEï¼Œç›´æ¥é‡å®šå‘åˆ°æ–‡ä»¶ ğŸ”¥ğŸ”¥ğŸ”¥
        with open(log_file, 'w') as f_out, open(err_file, 'w') as f_err:
            process = subprocess.Popen(
                cmd,
                env=env,
                stdout=f_out,  # ç›´æ¥å†™å…¥æ–‡ä»¶
                stderr=f_err   # ç›´æ¥å†™å…¥æ–‡ä»¶
            )
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸï¼Œè®¾ç½®5åˆ†é’Ÿè¶…æ—¶
            returncode = process.wait(timeout=300)
        
        elapsed = time.time() - start_time
        
        if returncode == 0:
            print(f"\nâœ… è®­ç»ƒæˆåŠŸ! (è€—æ—¶: {elapsed:.1f}ç§’)")
            print(f"è¯·æŸ¥çœ‹ {log_file} è·å–è¯¦ç»†è¾“å‡ºã€‚")
            return True
        else:
            print(f"\nâŒ è®­ç»ƒå¤±è´¥! (Exit Code: {returncode}, è€—æ—¶: {elapsed:.1f}ç§’)")
            print("é”™è¯¯æ—¥å¿—å†…å®¹å¦‚ä¸‹:")
            with open(err_file, 'r') as f:
                print(f.read() or "(é”™è¯¯æ—¥å¿—ä¸ºç©º)")
            return False

    except subprocess.TimeoutExpired:
        print("\nâŒ è®­ç»ƒè¶…æ—¶ (5åˆ†é’Ÿ)")
        if process:
            process.kill()
        print("è¿›ç¨‹å·²è¢«ç»ˆæ­¢ã€‚è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ä»¥ç¡®å®šå¡åœ¨ä½•å¤„:")
        print(f"tail -100 {log_file}")
        print(f"tail -100 {err_file}")
        return False
    except Exception as e:
        print(f"\nâŒ å¯åŠ¨è®­ç»ƒæ—¶å‘ç”Ÿè‡´å‘½å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        cleanup_resources()

if __name__ == "__main__":
    try:
        test_train_ddp()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        cleanup_resources(deep=True)
    finally:
        print("\nğŸ‘‹ æµ‹è¯•ç»“æŸ")
