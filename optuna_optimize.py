"""
Optuna è¶…å‚æ•°ä¼˜åŒ– - DDPå¤šGPUç‰ˆæœ¬ (ä¸‰é˜¶æ®µæ–¹æ¡ˆ) - æŠ¥é”™è·³è¿‡ç‰ˆæœ¬
"""
import os
os.environ['MKL_THREADING_LAYER'] = 'GNU'
os.environ['MKL_SERVICE_FORCE_INTEL'] = '1'
import sys
import subprocess
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler
import json
import pickle
import pandas as pd
import numpy as np
from datetime import datetime
import time
import warnings
import signal
warnings.filterwarnings('ignore')

# ===================== é…ç½® =====================
STUDY_NAME = "mamba2mil_survival_ddp_optimization"
STORAGE_PATH = "./results/optuna_study/optuna.db"
N_JOBS = 1

# DDPé…ç½®
NUM_GPUS = 2
CUDA_VISIBLE_DEVICES = "0,1"
MASTER_ADDR = "127.0.0.1"
BASE_PORT = 29500

# æ•°æ®è·¯å¾„
CSV_PATH = "/home/stat-jijianxin/PFMs/Survival_code/csv_file/hmu_survival_with_slides.csv"
H5_DIR = "/home/stat-jijianxin/PFMs/HMU_GC_ALL_H5/"
EXTERNAL_CSV = "/home/stat-jijianxin/PFMs/Survival_code/csv_file/tcga_survival_matched.csv"
EXTERNAL_H5 = "/home/stat-jijianxin/PFMs/TRIDENT/tcga_filtered/20x_512px_0px_overlap/"

# å›ºå®šå‚æ•°
FIXED_PARAMS = {
    # æ•°æ®
    'csv_path': CSV_PATH,
    'h5_dir': H5_DIR,
    'external_csv_path': EXTERNAL_CSV,
    'external_h5_dir': EXTERNAL_H5,
    
    # æ¨¡å‹åŸºç¡€
    'in_dim': 768,
    'n_classes': 4,
    'feature_models': 'ctranspath',
    
    # è®­ç»ƒç­–ç•¥
    'max_epochs': 100,
    'stop_epoch': 30,
    'warmup': 5,
    'patience': 15,
    'early_stop_delta': 0.0001,
    
    # æŸå¤±å‡½æ•°
    'loss': 'combined',
    'main_loss_type': 'nll',
    'alpha_surv': 0.365,
    'ranking_margin': 0.0,
    
    # æ•°æ®é›†åˆ’åˆ†
    'k_fold': 10,
    'val_ratio': 0.1,
    'test_ratio': 0.1,
    'seed': 123,
    'num_workers': 0,
}


# ===================== ç¯å¢ƒé…ç½®å‡½æ•° =====================
def setup_environment():
    """è®¾ç½®ç¼“å­˜è·¯å¾„å’ŒNCCLé…ç½®"""
    home_cache = os.path.expanduser("~/.cache")
    
    os.environ['HOME_CACHE'] = home_cache
    os.environ['TRITON_CACHE_DIR'] = f"{home_cache}/triton"
    os.environ['TORCH_COMPILE_CACHE_DIR'] = f"{home_cache}/torch_compile"
    os.environ['TRANSFORMERS_CACHE'] = f"{home_cache}/transformers"
    os.environ['HF_HOME'] = f"{home_cache}/huggingface"
    
    # åˆ›å»ºç›®å½•
    os.makedirs(os.environ['TRITON_CACHE_DIR'], exist_ok=True)
    os.makedirs(os.environ['TORCH_COMPILE_CACHE_DIR'], exist_ok=True)
    
    # NCCLé…ç½®
    os.environ['NCCL_SOCKET_IFNAME'] = 'lo'
    os.environ['NCCL_IB_DISABLE'] = '1'
    os.environ['NCCL_P2P_DISABLE'] = '0'
    os.environ['NCCL_SHM_DISABLE'] = '0'
    os.environ['NCCL_BLOCKING_WAIT'] = '1'
    os.environ['NCCL_ASYNC_ERROR_HANDLING'] = '1'
    os.environ['NCCL_DEBUG'] = 'WARN'
    os.environ['NCCL_TIMEOUT'] = '1800'
    
    print(f"âœ“ ç¼“å­˜è·¯å¾„å·²è®¾ç½®åˆ°: {home_cache}")


def cleanup_resources():
    """å¼ºåŒ–èµ„æºæ¸…ç†"""
    print("ğŸ§¹ å¼ºåŒ–èµ„æºæ¸…ç†...")
    
    # 1. æ€æ­»æ‰€æœ‰ç›¸å…³è¿›ç¨‹
    subprocess.run("pkill -9 -f 'torchrun' 2>/dev/null || true", shell=True)
    subprocess.run("pkill -9 -f 'train_ddp.py' 2>/dev/null || true", shell=True)
    subprocess.run("pkill -9 -f 'python.*train_ddp.py' 2>/dev/null || true", shell=True)
    
    # 2. æ¸…ç†CUDAç¼“å­˜
    subprocess.run("""
python3 << 'PYEOF'
import torch
import gc
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        with torch.cuda.device(i):
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
gc.collect()
PYEOF
""", shell=True)
    
    # 3. æ¸…ç†å…±äº«å†…å­˜
    subprocess.run("rm -rf /dev/shm/torch_* 2>/dev/null || true", shell=True)
    
    # 4. æ¸…ç†Tritonç¼“å­˜
    triton_cache = os.environ.get('TRITON_CACHE_DIR')
    if triton_cache:
        subprocess.run(f"rm -rf {triton_cache}/* 2>/dev/null || true", shell=True)
    
    # 5. æ¸…ç†/tmpä¸´æ—¶æ–‡ä»¶
    subprocess.run("rm -rf /tmp/triton_cache_rank_* 2>/dev/null || true", shell=True)
    subprocess.run("rm -rf /tmp/torch_* 2>/dev/null || true", shell=True)
    
    time.sleep(3)
    print("âœ“ èµ„æºæ¸…ç†å®Œæˆ")


def wait_for_port(port, max_wait=30):
    """ç­‰å¾…ç«¯å£é‡Šæ”¾"""
    waited = 0
    while waited < max_wait:
        result = subprocess.run(
            f"netstat -tuln 2>/dev/null | grep -q ':{port} '",
            shell=True,
            capture_output=True
        )
        if result.returncode != 0:
            return True
        
        print(f"â³ ç­‰å¾…ç«¯å£ {port} é‡Šæ”¾... ({waited}/{max_wait})")
        time.sleep(1)
        waited += 1
    
    print(f"âš ï¸  ç«¯å£ {port} ä»è¢«å ç”¨ï¼Œå¼ºåˆ¶æ¸…ç†...")
    subprocess.run(f"fuser -k {port}/tcp 2>/dev/null || true", shell=True)
    time.sleep(2)
    return True


# ===================== DDPè®­ç»ƒå‡½æ•° - æŠ¥é”™è·³è¿‡ç‰ˆæœ¬ =====================
def run_ddp_training(params, trial_number, fold=0):
    """è¿è¡ŒDDPè®­ç»ƒ - æŠ¥é”™å°±è·³è¿‡"""
    master_port = BASE_PORT + trial_number
    
    # ğŸ”¥ ä»»ä½•å¼‚å¸¸éƒ½ç›´æ¥è¿”å› Noneï¼Œè®© Optuna è·³è¿‡è¿™ä¸ª trial
    try:
        cleanup_resources()
        wait_for_port(master_port)
        
        results_dir = f"./results/optuna_study/trial_{trial_number}"
        os.makedirs(results_dir, exist_ok=True)
        
        log_file = os.path.join(results_dir, f"trial_{trial_number}_fold_{fold}.log")
        
        # æ„å»ºåŸºç¡€å‘½ä»¤
        base_cmd = [
            "torchrun",
            f"--nproc_per_node={NUM_GPUS}",
            f"--master_addr={MASTER_ADDR}",
            f"--master_port={master_port}",
            "--node_rank=0",
            "--nnodes=1",
            "train_ddp.py",
            "--csv_path", params['csv_path'],
            "--h5_dir", params['h5_dir'],
            "--external_csv_path", params['external_csv_path'],
            "--external_h5_dir", params['external_h5_dir'],
            "--in_dim", str(params['in_dim']),
            "--n_classes", str(params['n_classes']),
            "--dropout", str(params['dropout']),
            "--act", params['act'],
            "--mamba_layer", str(params['mamba_layer']),
            "--batch_size", str(params['batch_size']),
            "--max_epochs", str(params['max_epochs']),
            "--lr", str(params['lr']),
            "--weight_decay", str(params['weight_decay']),
            "--optimizer", params['optimizer'],
            "--loss", params['loss'],
            "--main_loss_type", params['main_loss_type'],
            "--alpha_surv", str(params['alpha_surv']),
            "--ranking_weight", str(params['ranking_weight']),
            "--ranking_margin", str(params['ranking_margin']),
            "--gc", str(params['gc']),
            "--k_fold", str(params['k_fold']),
            "--fold", str(fold),
            "--val_ratio", str(params['val_ratio']),
            "--test_ratio", str(params['test_ratio']),
            "--warmup", str(params['warmup']),
            "--patience", str(params['patience']),
            "--stop_epoch", str(params['stop_epoch']),
            "--results_dir", results_dir,
            "--num_workers", str(params['num_workers']),
            "--seed", str(params['seed']),
        ]
        
        # æ·»åŠ  feature_models
        if 'feature_models' in params:
            feature_models = params['feature_models']
            if isinstance(feature_models, list):
                for model in feature_models:
                    base_cmd.extend(["--feature_models", model])
            else:
                base_cmd.extend(["--feature_models", feature_models])
        
        # æ·»åŠ è°ƒåº¦å™¨å‚æ•°
        if 'scheduler' in params:
            base_cmd.extend(["--scheduler", params['scheduler']])
            if params['scheduler'] == 'cosine':
                base_cmd.extend(["--min_lr", str(params.get('min_lr', 1e-6))])
            elif params['scheduler'] == 'step':
                base_cmd.extend([
                    "--step_size", str(params.get('step_size', 30)),
                    "--gamma", str(params.get('gamma', 0.1))
                ])
        
        # æ„å»ºå®Œæ•´å‘½ä»¤
        cmd_str = ' '.join([f'"{arg}"' if ' ' in str(arg) else str(arg) for arg in base_cmd])
        full_cmd = f'bash -c "set -o pipefail; {cmd_str} 2>&1 | tee {log_file}"'
        
        env = os.environ.copy()
        env['CUDA_VISIBLE_DEVICES'] = CUDA_VISIBLE_DEVICES
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¯åŠ¨ Trial {trial_number} (Fold {fold})")
        print(f"ç«¯å£: {master_port}")
        print(f"æ—¥å¿—: {log_file}")
        print(f"{'='*60}\n")
        
        # å¯åŠ¨è¿›ç¨‹
        process = subprocess.Popen(
            full_cmd,
            shell=True,
            env=env,
            stdout=sys.stdout,
            stderr=subprocess.PIPE,
            text=True,
            preexec_fn=os.setsid
        )
        
        print(f"âœ“ è¿›ç¨‹å·²å¯åŠ¨ (PID: {process.pid})")
        
        # ç­‰å¾…å®Œæˆï¼ˆ4å°æ—¶è¶…æ—¶ï¼‰
        try:
            returncode = process.wait(timeout=14400)
        except subprocess.TimeoutExpired:
            print(f"\nâ±ï¸  è®­ç»ƒè¶…æ—¶ (4å°æ—¶)ï¼Œè·³è¿‡æ­¤ trial")
            try:
                os.killpg(os.getpgid(process.pid), signal.SIGKILL)
            except:
                pass
            cleanup_resources()
            return None
        
        # æ£€æŸ¥è¿”å›ç 
        if returncode != 0:
            print(f"\nâŒ è®­ç»ƒå¤±è´¥ (exitcode={returncode})ï¼Œè·³è¿‡æ­¤ trial")
            return None
        
        # è¯»å–ç»“æœ
        results_file = os.path.join(results_dir, f'fold_{fold}', 'results.pkl')
        if not os.path.exists(results_file):
            print(f"\nâŒ ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ­¤ trial")
            return None
        
        with open(results_file, 'rb') as f:
            results = pickle.load(f)
        
        val_cindex = results.get('val_cindex')
        print(f"\n{'='*60}")
        print(f"âœ… Trial {trial_number} å®Œæˆ")
        print(f"   éªŒè¯é›† C-index: {val_cindex:.4f}")
        print(f"{'='*60}\n")
        
        cleanup_resources()
        time.sleep(5)
        
        return val_cindex
    
    except Exception as e:
        # ğŸ”¥ ä»»ä½•å¼‚å¸¸éƒ½æ‰“å°åè·³è¿‡
        print(f"\nâš ï¸  Trial {trial_number} å‘ç”Ÿå¼‚å¸¸ï¼Œè·³è¿‡: {e}")
        
        # æ¸…ç†è¿›ç¨‹
        try:
            subprocess.run("pkill -9 -f 'torchrun' 2>/dev/null || true", shell=True)
            subprocess.run("pkill -9 -f 'train_ddp.py' 2>/dev/null || true", shell=True)
        except:
            pass
        
        cleanup_resources()
        return None


# ===================== Optunaç›®æ ‡å‡½æ•° =====================
def objective_stage1(trial):
    """é˜¶æ®µ1: æ ¸å¿ƒæ¶æ„"""
    params = FIXED_PARAMS.copy()
    
    params.update({
        # ğŸ”¥ dropout æ”¹ä¸ºç¦»æ•£é€‰æ‹©ï¼ŒèŒƒå›´è°ƒé«˜
        'dropout': trial.suggest_categorical('dropout', [0.6, 0.7, 0.8]),
        
        'act': trial.suggest_categorical('act', ['relu', 'gelu']),
        
        # ğŸ”¥ mamba_layer æœ¬æ¥å°±æ˜¯ç¦»æ•£çš„
        'mamba_layer': trial.suggest_int('mamba_layer', 1, 4),
        
        'batch_size': trial.suggest_categorical('batch_size', [4, 8, 16]),
        
        # ğŸ”¥ lr æ”¹ä¸ºç¦»æ•£é€‰æ‹©ï¼ˆå¯¹æ•°ç©ºé—´ï¼‰
        'lr': trial.suggest_categorical('lr', [1e-5, 1e-4]),
        
        # ğŸ”¥ weight_decay æ”¹ä¸ºç¦»æ•£é€‰æ‹©
        'weight_decay': trial.suggest_categorical('weight_decay', [1e-6, 1e-5, 1e-4]),
        
        'optimizer': trial.suggest_categorical('optimizer', ['adam', 'adamw']),
        
        # ğŸ”¥ ranking_weight æ”¹ä¸ºç¦»æ•£é€‰æ‹©ï¼ˆç²¾ç®€ï¼‰
        'ranking_weight': trial.suggest_categorical('ranking_weight', [0.0, 0.1, 0.2, 0.3]),
        
        # ğŸ”¥ gc ç²¾ç®€åˆ°å…³é”®å€¼ï¼ˆæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ä¸€èˆ¬ä¸éœ€è¦å¤ªå¤šé€‰æ‹©ï¼‰
        'gc': trial.suggest_categorical('gc', [8, 16, 32]),
    })
    
    print(f"\n{'#'*60}")
    print(f"# Trial {trial.number} å‚æ•°:")
    print(f"{'#'*60}")
    for key, value in params.items():
        if key in trial.params:
            print(f"  {key}: {value}")
    print(f"{'#'*60}\n")
    
    val_cindex = run_ddp_training(params, trial.number)
    
    # ğŸ”¥ è¿”å› None å°±è·³è¿‡
    if val_cindex is None:
        raise optuna.TrialPruned()
    
    return val_cindex


def objective_stage2(trial, best_stage1_params):
    """é˜¶æ®µ2: æŸå¤±å‡½æ•°+è°ƒåº¦å™¨"""
    params = FIXED_PARAMS.copy()
    params.update(best_stage1_params)
    
    scheduler = trial.suggest_categorical('scheduler', ['cosine', 'step', 'plateau'])
    params['scheduler'] = scheduler
    
    if scheduler == 'cosine':
        params['min_lr'] = trial.suggest_float('min_lr', 1e-7, 1e-5, log=True)
    elif scheduler == 'step':
        params['step_size'] = trial.suggest_int('step_size', 20, 50)
        params['gamma'] = trial.suggest_float('gamma', 0.1, 0.5)
    
    params['ranking_weight'] = trial.suggest_float('ranking_weight', 0.0, 0.5)
    
    print(f"\n{'#'*60}")
    print(f"# Trial {trial.number} å‚æ•° (é˜¶æ®µ2):")
    print(f"{'#'*60}")
    for key, value in params.items():
        if key in trial.params:
            print(f"  {key}: {value}")
    print(f"{'#'*60}\n")
    
    val_cindex = run_ddp_training(params, trial.number)
    
    if val_cindex is None:
        raise optuna.TrialPruned()
    
    return val_cindex


def objective_stage3(trial, best_stage2_params):
    """é˜¶æ®µ3: æ­£åˆ™åŒ–å¾®è°ƒ"""
    params = FIXED_PARAMS.copy()
    params.update(best_stage2_params)
    
    # ğŸ”¥ ä¿®å¤ dropout èŒƒå›´é—®é¢˜
    dropout_center = best_stage2_params['dropout']
    params['dropout'] = trial.suggest_float('dropout', 
        max(0.3, dropout_center - 0.1),
        min(0.9, dropout_center + 0.1)
    )
    
    params['weight_decay'] = trial.suggest_float('weight_decay',
        best_stage2_params['weight_decay'] * 0.5,
        best_stage2_params['weight_decay'] * 2.0,
        log=True
    )
    
    params['gc'] = trial.suggest_int('gc',
        max(1, best_stage2_params['gc'] - 8),
        min(32, best_stage2_params['gc'] + 8)
    )
    
    print(f"\n{'#'*60}")
    print(f"# Trial {trial.number} å‚æ•° (é˜¶æ®µ3):")
    print(f"{'#'*60}")
    for key, value in params.items():
        if key in trial.params:
            print(f"  {key}: {value}")
    print(f"{'#'*60}\n")
    
    val_cindex = run_ddp_training(params, trial.number)
    
    if val_cindex is None:
        raise optuna.TrialPruned()
    
    return val_cindex


# ===================== ä¸»å‡½æ•° =====================
def main():
    """ä¸‰é˜¶æ®µä¼˜åŒ–ä¸»æµç¨‹"""
    
    setup_environment()
    
    start_time = time.time()
    
    os.makedirs("./results/optuna_study", exist_ok=True)
    
    storage = f"sqlite:///{STORAGE_PATH}"
    
    print("\n" + "="*60)
    print("ğŸ¯ é˜¶æ®µ1: æ ¸å¿ƒæ¶æ„ä¼˜åŒ–")
    print("="*60)
    
    study1 = optuna.create_study(
        study_name=f"{STUDY_NAME}_stage1",
        storage=storage,
        load_if_exists=True,
        direction="maximize",
        sampler=TPESampler(seed=42),
        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)
    )
    
    study1.optimize(objective_stage1, n_trials=30, n_jobs=N_JOBS)
    
    best_stage1 = study1.best_trial
    print(f"\n{'='*60}")
    print(f"âœ… é˜¶æ®µ1æœ€ä½³ç»“æœ:")
    print(f"{'='*60}")
    print(f"   C-Index: {best_stage1.value:.4f}")
    print(f"   å‚æ•°:")
    for key, value in best_stage1.params.items():
        print(f"     {key}: {value}")
    print(f"{'='*60}\n")
    
    with open("./results/optuna_study/stage1_best.json", 'w') as f:
        json.dump({
            'value': best_stage1.value,
            'params': best_stage1.params
        }, f, indent=2)
    
    print("\n" + "="*60)
    print("ğŸ¯ é˜¶æ®µ2: æŸå¤±å‡½æ•°+è°ƒåº¦å™¨ä¼˜åŒ–")
    print("="*60)
    
    study2 = optuna.create_study(
        study_name=f"{STUDY_NAME}_stage2",
        storage=storage,
        load_if_exists=True,
        direction="maximize",
        sampler=TPESampler(seed=42),
        pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=5)
    )
    
    study2.optimize(
        lambda trial: objective_stage2(trial, best_stage1.params),
        n_trials=20,
        n_jobs=N_JOBS
    )
    
    best_stage2 = study2.best_trial
    print(f"\n{'='*60}")
    print(f"âœ… é˜¶æ®µ2æœ€ä½³ç»“æœ:")
    print(f"{'='*60}")
    print(f"   C-Index: {best_stage2.value:.4f}")
    print(f"   å‚æ•°:")
    for key, value in best_stage2.params.items():
        print(f"     {key}: {value}")
    print(f"{'='*60}\n")
    
    final_params = FIXED_PARAMS.copy()
    final_params.update(best_stage1.params)
    final_params.update(best_stage2.params)
    
    with open("./results/optuna_study/stage2_best.json", 'w') as f:
        json.dump({
            'value': best_stage2.value,
            'params': best_stage2.params,
            'full_params': final_params
        }, f, indent=2)
    
    print("\n" + "="*60)
    print("ğŸ¯ é˜¶æ®µ3: æ­£åˆ™åŒ–å¾®è°ƒ")
    print("="*60)
    
    study3 = optuna.create_study(
        study_name=f"{STUDY_NAME}_stage3",
        storage=storage,
        load_if_exists=True,
        direction="maximize",
        sampler=TPESampler(seed=42),
        pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=5)
    )
    
    study3.optimize(
        lambda trial: objective_stage3(trial, final_params),
        n_trials=15,
        n_jobs=N_JOBS
    )
    
    best_stage3 = study3.best_trial
    print(f"\n{'='*60}")
    print(f"âœ… é˜¶æ®µ3æœ€ä½³ç»“æœ:")
    print(f"{'='*60}")
    print(f"   C-Index: {best_stage3.value:.4f}")
    print(f"   å‚æ•°:")
    for key, value in best_stage3.params.items():
        print(f"     {key}: {value}")
    print(f"{'='*60}\n")
    
    final_best_params = FIXED_PARAMS.copy()
    final_best_params.update(best_stage1.params)
    final_best_params.update(best_stage2.params)
    final_best_params.update(best_stage3.params)
    
    with open("./results/optuna_study/final_best.json", 'w') as f:
        json.dump({
            'value': best_stage3.value,
            'params': best_stage3.params,
            'full_params': final_best_params
        }, f, indent=2)
    
    elapsed = time.time() - start_time
    hours = int(elapsed // 3600)
    minutes = int((elapsed % 3600) // 60)
    seconds = int(elapsed % 60)
    
    print("\n" + "="*60)
    print("ğŸ‰ ä¸‰é˜¶æ®µä¼˜åŒ–å®Œæˆ!")
    print("="*60)
    print(f"æ€»è€—æ—¶: {hours}h {minutes}m {seconds}s")
    print(f"\næœ€ç»ˆæœ€ä½³å‚æ•°:")
    print(json.dumps(final_best_params, indent=2))
    print(f"\næœ€ä½³ C-Index: {best_stage3.value:.4f}")
    print("="*60)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨æ¸…ç†èµ„æº...")
        cleanup_resources()
        print("âœ“ æ¸…ç†å®Œæˆ")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        cleanup_resources()
        sys.exit(1)
